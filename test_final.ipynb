{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_null_columns(df, threshold=0.5):\n",
    "    # Calculate the percentage of null values for each column\n",
    "    null_percentages = df.isnull().mean()\n",
    "    # Identify columns where the percentage of null values exceeds the threshold\n",
    "    cols_to_drop = null_percentages[null_percentages > threshold].index.tolist()\n",
    "    # Drop the identified columns from the DataFrame\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    @staticmethod\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_cols(df):\n",
    "        # List of attributes to exclude\n",
    "\n",
    "        # manual select the columns to remove\n",
    "        excluded_attributes = [\n",
    "            \"addres_district_368M\", \"addres_role_871L\", \"addres_zip_823M\",\n",
    "            \"amtinstpaidbefduel24m_4187115A\", \"annualeffectiverate_199L\", \"annuity_853A\",\n",
    "            \"applicationcnt_361L\", \"applications30d_658L\", \"applicationscnt_1086L\",\n",
    "            \"applicationscnt_464L\", \"applicationscnt_629L\", \"applicationscnt_867L\",\n",
    "            \"approvaldate_319D\", \"assignmentdate_238D\", \"assignmentdate_4527235D\",\n",
    "            \"assignmentdate_4955616D\", \"bankacctype_710L\", \"birth_259D\", \"birthdate_574D\",\n",
    "            \"birthdate_87D\", \"byoccupationinc_3656910L\", \"cardtype_51L\", \"childnum_21L\",\n",
    "            \"classificationofcontr_1114M\", \"classificationofcontr_13M\", \"classificationofcontr_400M\",\n",
    "            \"clientscnt_100L\", \"clientscnt_1022L\", \"clientscnt_1071L\", \"clientscnt_1130L\",\n",
    "            \"clientscnt_136L\", \"clientscnt_157L\", \"clientscnt_257L\", \"clientscnt_304L\",\n",
    "            \"clientscnt_360L\", \"clientscnt_493L\", \"clientscnt_533L\", \"clientscnt_887L\",\n",
    "            \"clientscnt_946L\", \"clientscnt12m_3712952L\", \"clientscnt3m_3712950L\",\n",
    "            \"clientscnt6m_3712949L\", \"cntpmts24_3658933L\", \"collater_typofvalofguarant_298M\",\n",
    "            \"collater_typofvalofguarant_407M\", \"collaterals_typeofguarante_359M\",\n",
    "            \"collaterals_typeofguarante_669M\", \"contaddr_district_15M\", \"contaddr_matchlist_1032L\",\n",
    "            \"contaddr_smempladdr_334L\", \"contaddr_zipcode_807M\", \"contractdate_551D\",\n",
    "            \"contractenddate_991D\", \"contractmaturitydate_151D\", \"contractst_964M\",\n",
    "            \"contractsum_5085717L\", \"contracttype_653M\", \"conts_role_79M\", \"conts_type_509L\",\n",
    "            \"creationdate_885D\", \"credlmt_228A\", \"credlmt_230A\", \"credor_3940957M\", \"credtype_322L\",\n",
    "            \"credtype_587L\", \"dateactivated_425D\", \"datefirstoffer_1144D\", \"datelastinstal40dpd_247D\",\n",
    "            \"datelastunpaid_3546854D\", \"dateofcredend_289D\", \"dateofcredend_353D\", \"dateofcredstart_181D\",\n",
    "            \"dateofcredstart_739D\", \"dateofrealrepmt_138D\", \"deductiondate_4917603D\", \"description_351M\",\n",
    "            \"description_5085714M\", \"disbursementtype_67L\", \"district_544M\", \"dpdmaxdatemonth_804T\",\n",
    "            \"dpdmaxdatemonth_89T\", \"dpdmaxdateyear_596T\", \"dpdmaxdateyear_742T\", \"dpdmaxdateyear_896T\",\n",
    "            \"dtlastpmt_581D\", \"dtlastpmtallstes_3545839D\", \"dtlastpmtallstes_4499206D\", \"education_1103M\",\n",
    "            \"education_1138M\", \"education_88M\", \"education_927M\", \"eir_270L\", \"empl_employedfrom_271D\",\n",
    "            \"empladdr_district_926M\", \"empladdr_zipcode_114M\", \"employedfrom_700D\", \"employername_160M\",\n",
    "            \"empls_employedfrom_796D\", \"empls_employer_name_740M\", \"familystate_447L\", \"familystate_726L\",\n",
    "            \"financialinstitution_382M\", \"financialinstitution_591M\", \"firstclxcampaign_1125D\",\n",
    "            \"firstdatedue_489D\", \"firstnonzeroinstldate_307D\", \"fourthquarter_440L\", \"gender_992L\",\n",
    "            \"housetype_905L\", \"housingtype_772L\", \"incometype_1044T\", \"inittransactioncode_186L\",\n",
    "            \"inittransactioncode_279L\", \"isbidproduct_1095L\", \"isbidproduct_390L\", \"isbidproductrequest_292L\",\n",
    "            \"isdebitcard_527L\", \"isdebitcard_729L\", \"language1_981M\", \"last180dayturnover_1134A\",\n",
    "            \"last30dayturnover_651A\", \"lastactivateddate_801D\", \"lastapplicationdate_877D\",\n",
    "            \"lastapprcommoditycat_1041M\", \"lastapprcommoditytypec_5251766M\", \"lastapprdate_640D\",\n",
    "            \"lastcancelreason_561M\", \"lastdelinqdate_224D\", \"lastrejectcommoditycat_161M\",\n",
    "            \"lastrejectcommodtypec_5251769M\", \"lastrejectdate_50D\", \"lastrejectreason_759M\",\n",
    "            \"lastrejectreasonclient_4145040M\", \"lastrepayingdate_696D\", \"lastupdate_1112D\",\n",
    "            \"lastupdate_260D\", \"lastupdate_388D\", \"maritalst_385M\", \"maritalst_703L\", \"maritalst_893M\",\n",
    "            \"maxdpdinstldate_3546855D\", \"mobilephncnt_593L\", \"name_4527232M\", \"name_4917606M\",\n",
    "            \"numberofoverdueinstlmaxdat_148D\", \"numberofoverdueinstlmaxdat_641D\", \"openingdate_313D\",\n",
    "            \"openingdate_857D\", \"overdueamountmax2date_1002D\", \"overdueamountmax2date_1142D\",\n",
    "            \"overdueamountmaxdatemonth_284T\", \"overdueamountmaxdatemonth_365T\", \"overdueamountmaxdatemonth_494T\",\n",
    "            \"overdueamountmaxdateyear_2T\", \"overdueamountmaxdateyear_432T\", \"overdueamountmaxdateyear_994T\",\n",
    "            \"paytype_783L\", \"paytype1st_925L\", \"payvacationpostpone_4187118D\", \"periodicityofpmts_1102L\",\n",
    "            \"periodicityofpmts_837L\", \"personindex_1023L\", \"persontype_1072L\", \"persontype_792L\",\n",
    "            \"posfpd10lastmonth_333P\", \"posfpd30lastmonth_3976960P\", \"posfstqpd30lastmonth_3976962P\",\n",
    "            \"postype_4733339M\", \"previouscontdistrict_112M\", \"processingdate_168D\", \"purposeofcred_426M\",\n",
    "            \"purposeofcred_722M\", \"purposeofcred_874M\", \"registaddr_district_1083M\", \"registaddr_zipcode_184M\",\n",
    "            \"rejectreason_755M\", \"rejectreasonclient_4145042M\", \"relatedpersons_role_762T\",\n",
    "            \"relationshiptoclient_415T\", \"relationshiptoclient_642T\", \"requesttype_4525192L\",\n",
    "            \"responsedate_1012D\", \"responsedate_4527233D\", \"responsedate_4917613D\", \"role_1084L\",\n",
    "            \"role_993L\", \"secondquarter_766L\", \"sellerplacecnt_915L\", \"sellerplacescnt_216L\",\n",
    "            \"sex_738L\", \"subjectrole_182M\", \"subjectrole_326M\", \"subjectrole_43M\", \"subjectrole_93M\",\n",
    "            \"thirdquarter_1082L\", \"twobodfilling_608L\", \"type_25L\", \"typesuite_864L\", \"validfrom_1069D\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        # Exclude the attributes\n",
    "        df = df.drop(excluded_attributes, errors='ignore')\n",
    "    \n",
    "    #        # Print the number of columns left\n",
    "    #     print(\"Number of columns left after excluding:\", len(df.columns))\n",
    "\n",
    "    #    # Preprocess the dataframe\n",
    "        df = drop_high_null_columns(df)\n",
    "    #     # Print the number of columns left\n",
    "    #     print(\"Number of columns left after droping the null:\", len(df.columns))\n",
    "    #     #df = drop_low_high_cardinality_columns(df)\n",
    "\n",
    "    #     # print the number of columns left\n",
    "    #     #print(\"Number of columns left after droping the low and high cardinality columns:\", len(df.columns))\n",
    "\n",
    "    #     # Print the columns that have null values\n",
    "    #     null_columns = [col for col in df.columns if df[col].null_count() > 0]\n",
    "    #     print(\"Columns with null values:\", null_columns)\n",
    "\n",
    "    #             # Print the number of columns left\n",
    "    #     print(\"Number of columns left after dropping the low and high cardinality columns:\", len(df.columns))\n",
    "\n",
    "        # Print the null values for each column\n",
    "        for col in df.columns:\n",
    "            null_count = df[col].isnull().sum()  # Count the number of null values\n",
    "            print(f\"Null values in '{col}': {null_count}\")\n",
    "\n",
    "        # Impute missing values\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object' or df[col].dtype == 'category':\n",
    "                if not df[col].mode().empty:\n",
    "                    mode = df[col].mode().iloc[0]  # Calculate the mode\n",
    "                    df[col].fillna(mode, inplace=True)  # Fill null values with the mode inplace\n",
    "                else:\n",
    "                    print(f\"No mode available for column '{col}'. Skipping imputation for this column.\")\n",
    "            else:\n",
    "                if not df[col].isnull().all():\n",
    "                    mean = df[col].mean()  # Calculate the mean\n",
    "                    df[col].fillna(mean, inplace=True)  # Fill null values with the mean inplace\n",
    "                else:\n",
    "                    print(f\"All values are null for column '{col}'. Skipping imputation for this column.\")\n",
    "\n",
    "        # Print the null values for each column after imputation\n",
    "        for col in df.columns:\n",
    "            null_count = df[col].isnull().sum()  # Count the number of null values\n",
    "            print(f\"Null values in '{col}' after imputation: {null_count}\")\n",
    "\n",
    "        # Print the number of columns left\n",
    "        print(\"Number of columns left after imputation:\", len(df.columns))\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        \n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        \n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    \n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        \n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        \n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "        \n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    \n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    \n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    \n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    \n",
    "    return df_data, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "##TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "##TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "from pathlib import Path\n",
    "TRAIN_DIR = Path(\"../parquet_files/train\")\n",
    "TEST_DIR = Path(\"../parquet_files/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = feature_eng(**data_store)\n",
    "\n",
    "print(\"train data shape:\\t\", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = feature_eng(**data_store)\n",
    "\n",
    "print(\"test data shape:\\t\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data to pandas and filter\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train_filtered = df_train.pipe(Pipeline.filter_cols)\n",
    "\n",
    "# Convert test data to pandas and filter\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "df_test_filtered = df_test.pipe(Pipeline.filter_cols)\n",
    "\n",
    "# Ensure that both train and test data have the same columns except for the target attribute\n",
    "common_columns = list(set(df_train_filtered.columns).intersection(set(df_test_filtered.columns)))\n",
    "\n",
    "# Add specific columns to the common columns list if they are not already present\n",
    "columns_to_keep = ['case_id', 'WEEK_NUM', 'month_decision', 'weekday_decision']\n",
    "for col in columns_to_keep:\n",
    "    if col not in common_columns:\n",
    "        common_columns.append(col)\n",
    "\n",
    "# Preserve the original order of columns in df_train_filtered\n",
    "common_columns = [col for col in df_train_filtered.columns if col in common_columns]\n",
    "\n",
    "# Add 'target' column to train data\n",
    "df_train_filtered = df_train_filtered[common_columns + ['target']]\n",
    "\n",
    "# Filter test data to include only common columns\n",
    "df_test_filtered = df_test_filtered[common_columns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train, cat_cols = to_pandas(df_train)\n",
    "# df_test, cat_cols = to_pandas(df_test, cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del data_store\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train is duplicated:\\t\", df_train_filtered[\"case_id\"].duplicated().any())\n",
    "print(\"Train Week Range:\\t\", (df_train_filtered[\"WEEK_NUM\"].min(), df_train[\"WEEK_NUM\"].max()))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Test is duplicated:\\t\", df_test_filtered[\"case_id\"].duplicated().any())\n",
    "print(\"Test Week Range:\\t\", (df_test_filtered[\"WEEK_NUM\"].min(), df_test[\"WEEK_NUM\"].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(\n",
    "#     data=df_train,\n",
    "#     x=\"WEEK_NUM\",\n",
    "#     y=\"target\",\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_filtered.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "y = df_train_filtered[\"target\"]\n",
    "weeks = df_train_filtered[\"WEEK_NUM\"]\n",
    "\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"colsample_bytree\": 0.8, \n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"class_weight\": \"balanced\",\n",
    "    #\"device\": \"gpu\",\n",
    "}\n",
    "\n",
    "fitted_models = []\n",
    "\n",
    "for idx_train, idx_valid in cv.split(X, y, groups=weeks):\n",
    "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)]\n",
    "    )\n",
    "\n",
    "    fitted_models.append(model)\n",
    "\n",
    "model = VotingModel(fitted_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test_filtered.drop(columns=[\"WEEK_NUM\"])\n",
    "X_test = X_test.set_index(\"case_id\")\n",
    "\n",
    "y_pred = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm = pd.read_csv(\"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "df_subm[\"score\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check null: \", df_subm[\"score\"].isnull().any())\n",
    "\n",
    "df_subm_sorted = df_subm.sort_values(by=\"score\", ascending=False)\n",
    "top_5_scores = df_subm_sorted.head()\n",
    "print(top_5_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
