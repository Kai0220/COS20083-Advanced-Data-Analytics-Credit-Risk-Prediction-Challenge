{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import linregress\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimators):\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pre-processing functions\n",
    "def drop_high_null_columns(df, threshold=0.5):\n",
    "    # Calculate the percentage of null values for each column\n",
    "    null_percentages = df.isnull().mean()\n",
    "    # Identify columns where the percentage of null values exceeds the threshold\n",
    "    cols_to_drop = null_percentages[null_percentages > threshold].index.tolist()\n",
    "    # Drop the identified columns from the DataFrame\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    @staticmethod\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))            \n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
    "                df = df.with_columns(pl.col(col).dt.total_days())\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "                \n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_cols(df):\n",
    "        # List of attributes to exclude\n",
    "\n",
    "        # manual select the columns to remove \n",
    "        excluded_attributes = [\n",
    "         \"addres_district_368M\", \"addres_role_871L\", \"addres_zip_823M\",\n",
    "         \"amtinstpaidbefduel24m_4187115A\", \"annualeffectiverate_199L\", \"annuity_853A\",\n",
    "         \"applicationcnt_361L\", \"applications30d_658L\", \"applicationscnt_1086L\",\n",
    "         \"applicationscnt_464L\", \"applicationscnt_629L\", \"applicationscnt_867L\",\n",
    "         \"approvaldate_319D\", \"assignmentdate_238D\", \"assignmentdate_4527235D\",\n",
    "         \"assignmentdate_4955616D\", \"bankacctype_710L\", \"birth_259D\", \"birthdate_574D\",\n",
    "         \"birthdate_87D\", \"byoccupationinc_3656910L\", \"cardtype_51L\", \"childnum_21L\",\n",
    "         \"classificationofcontr_1114M\", \"classificationofcontr_13M\", \"classificationofcontr_400M\",\n",
    "         \"clientscnt_100L\", \"clientscnt_1022L\", \"clientscnt_1071L\", \"clientscnt_1130L\",\n",
    "         \"clientscnt_136L\", \"clientscnt_157L\", \"clientscnt_257L\", \"clientscnt_304L\",\n",
    "         \"clientscnt_360L\", \"clientscnt_493L\", \"clientscnt_533L\", \"clientscnt_887L\",\n",
    "         \"clientscnt_946L\", \"clientscnt12m_3712952L\", \"clientscnt3m_3712950L\",\n",
    "         \"clientscnt6m_3712949L\", \"cntpmts24_3658933L\", \"collater_typofvalofguarant_298M\",\n",
    "         \"collater_typofvalofguarant_407M\", \"collaterals_typeofguarante_359M\",\n",
    "         \"collaterals_typeofguarante_669M\", \"contaddr_district_15M\", \"contaddr_matchlist_1032L\",\n",
    "         \"contaddr_smempladdr_334L\", \"contaddr_zipcode_807M\", \"contractdate_551D\",\n",
    "         \"contractenddate_991D\", \"contractmaturitydate_151D\", \"contractst_964M\",\n",
    "         \"contractsum_5085717L\", \"contracttype_653M\", \"conts_role_79M\", \"conts_type_509L\",\n",
    "         \"creationdate_885D\", \"credlmt_228A\", \"credlmt_230A\", \"credor_3940957M\", \"credtype_322L\",\n",
    "         \"credtype_587L\", \"dateactivated_425D\", \"datefirstoffer_1144D\", \"datelastinstal40dpd_247D\",\n",
    "         \"datelastunpaid_3546854D\", \"dateofcredend_289D\", \"dateofcredend_353D\", \"dateofcredstart_181D\",\n",
    "         \"dateofcredstart_739D\", \"dateofrealrepmt_138D\", \"deductiondate_4917603D\", \"description_351M\",\n",
    "         \"description_5085714M\", \"disbursementtype_67L\", \"district_544M\", \"dpdmaxdatemonth_804T\",\n",
    "         \"dpdmaxdatemonth_89T\", \"dpdmaxdateyear_596T\", \"dpdmaxdateyear_742T\", \"dpdmaxdateyear_896T\",\n",
    "         \"dtlastpmt_581D\", \"dtlastpmtallstes_3545839D\", \"dtlastpmtallstes_4499206D\", \"education_1103M\",\n",
    "         \"education_1138M\", \"education_88M\", \"education_927M\", \"eir_270L\", \"empl_employedfrom_271D\",\n",
    "         \"empladdr_district_926M\", \"empladdr_zipcode_114M\", \"employedfrom_700D\", \"employername_160M\",\n",
    "         \"empls_employedfrom_796D\", \"empls_employer_name_740M\", \"familystate_447L\", \"familystate_726L\",\n",
    "         \"financialinstitution_382M\", \"financialinstitution_591M\", \"firstclxcampaign_1125D\",\n",
    "         \"firstdatedue_489D\", \"firstnonzeroinstldate_307D\", \"fourthquarter_440L\", \"gender_992L\",\n",
    "         \"housetype_905L\", \"housingtype_772L\", \"incometype_1044T\", \"inittransactioncode_186L\",\n",
    "         \"inittransactioncode_279L\", \"isbidproduct_1095L\", \"isbidproduct_390L\", \"isbidproductrequest_292L\",\n",
    "         \"isdebitcard_527L\", \"isdebitcard_729L\", \"language1_981M\", \"last180dayturnover_1134A\",\n",
    "         \"last30dayturnover_651A\", \"lastactivateddate_801D\", \"lastapplicationdate_877D\",\n",
    "         \"lastapprcommoditycat_1041M\", \"lastapprcommoditytypec_5251766M\", \"lastapprdate_640D\",\n",
    "         \"lastcancelreason_561M\", \"lastdelinqdate_224D\", \"lastrejectcommoditycat_161M\",\n",
    "         \"lastrejectcommodtypec_5251769M\", \"lastrejectdate_50D\", \"lastrejectreason_759M\",\n",
    "         \"lastrejectreasonclient_4145040M\", \"lastrepayingdate_696D\", \"lastupdate_1112D\",\n",
    "         \"lastupdate_260D\", \"lastupdate_388D\", \"maritalst_385M\", \"maritalst_703L\", \"maritalst_893M\",\n",
    "         \"maxdpdinstldate_3546855D\", \"mobilephncnt_593L\", \"name_4527232M\", \"name_4917606M\",\n",
    "         \"numberofoverdueinstlmaxdat_148D\", \"numberofoverdueinstlmaxdat_641D\", \"openingdate_313D\",\n",
    "         \"openingdate_857D\", \"overdueamountmax2date_1002D\", \"overdueamountmax2date_1142D\",\n",
    "         \"overdueamountmaxdatemonth_284T\", \"overdueamountmaxdatemonth_365T\", \"overdueamountmaxdatemonth_494T\",\n",
    "         \"overdueamountmaxdateyear_2T\", \"overdueamountmaxdateyear_432T\", \"overdueamountmaxdateyear_994T\",\n",
    "         \"paytype_783L\", \"paytype1st_925L\", \"payvacationpostpone_4187118D\", \"periodicityofpmts_1102L\",\n",
    "         \"periodicityofpmts_837L\", \"personindex_1023L\", \"persontype_1072L\", \"persontype_792\"]\n",
    "\n",
    "        # Exclude the attributes\n",
    "        df = df.drop(excluded_attributes, errors='ignore')\n",
    "\n",
    "        # Preprocess the dataframe\n",
    "        df = drop_high_null_columns(df)\n",
    "        # df = drop_high_cardinality_columns(df)\n",
    "  \n",
    "        # Print the null values for each column\n",
    "        for col in df.columns:\n",
    "            null_count = df[col].isnull().sum()  # Count the number of null values\n",
    "            print(f\"Null values in '{col}': {null_count}\")\n",
    "\n",
    "       # Impute missing values (references to support)\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object' or df[col].dtype == 'category':\n",
    "                if not df[col].mode().empty:\n",
    "                    mode = df[col].mode().iloc[0]  # Calculate the mode\n",
    "                    df[col].fillna(mode, inplace=True)  # Fill null values with the mode inplace\n",
    "                    print(f\"Imputed missing values in '{col}' with mode ('{mode}').\")\n",
    "                else:\n",
    "                    print(f\"No mode available for column '{col}'. Skipping imputation for this column.\")\n",
    "            elif df[col].dtype in ['int64', 'float64']:\n",
    "                # Check the skewness of the column\n",
    "                skewness = df[col].skew()\n",
    "                if skewness < -1 or skewness > 1:  # Highly skewed\n",
    "                    median = df[col].median()\n",
    "                    df[col].fillna(median, inplace=True)\n",
    "                    print(f\"Imputed missing values in '{col}' with median ({median}) due to high skewness ({skewness}).\")\n",
    "                else:  # Normal or slightly skewed\n",
    "                    mean = df[col].mean()\n",
    "                    df[col].fillna(mean, inplace=True)\n",
    "                    print(f\"Imputed missing values in '{col}' with mean ({mean}).\")\n",
    "    \n",
    "\n",
    "        # Print the null values for each column after imputation\n",
    "        for col in df.columns:\n",
    "            null_count = df[col].isnull().sum()  # Count the number of null values\n",
    "            print(f\"Null values in '{col}' after imputation: {null_count}\")\n",
    "\n",
    "        # Print the number of columns left\n",
    "        print(\"Number of columns left after imputation:\", len(df.columns))\n",
    "\n",
    "        # Print the number of rows and columns\n",
    "        print(f\"Number of rows and columns: {df.shape}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    @staticmethod\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\",)]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        \n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        \n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max\n",
    "\n",
    "    @staticmethod\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    \n",
    "    if depth in [1, 2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        \n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        \n",
    "        chunks.append(df)\n",
    "        \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def calculate_age_from_year(year):\n",
    "    if year is None:\n",
    "        return None\n",
    "    current_year = datetime.today().year\n",
    "    return current_year - year\n",
    "\n",
    "def combine_education_levels(edu1, edu2):\n",
    "    # Example combination logic: take the maximum education level\n",
    "    # Modify this logic based on your specific needs and data characteristics\n",
    "    if edu1 is None and edu2 is None:\n",
    "        return None\n",
    "    elif edu1 is None:\n",
    "        return edu2\n",
    "    elif edu2 is None:\n",
    "        return edu1\n",
    "    else:\n",
    "        return max(edu1, edu2)\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    # Add basic date features\n",
    "    df_base = df_base.with_columns(\n",
    "        pl.col(\"date_decision\").dt.month().alias(\"month_decision\"),\n",
    "        pl.col(\"date_decision\").dt.weekday().alias(\"weekday_decision\"),\n",
    "    )\n",
    "    \n",
    "    # Join additional dataframes\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "        \n",
    "    # Feature engineering: Sum of credit limits if they exist\n",
    "    credit_limit_cols = [\"credlmt_1052A\", \"credlmt_3940954A\", \"credlmt_935A\"]\n",
    "    if all(col_name in df_base.columns for col_name in credit_limit_cols):\n",
    "        df_base = df_base.with_columns(\n",
    "            (pl.col(credit_limit_cols).sum()).alias(\"total_credit_limit\")\n",
    "        )\n",
    "    \n",
    "    # Feature engineering: Deposit Ratio\n",
    "    deposit_incoming_col = \"amtdepositincoming_4809444A\"\n",
    "    deposit_outgoing_col = \"amtdepositoutgoing_4809442A\"\n",
    "    \n",
    "    if deposit_incoming_col in df_base.columns and deposit_outgoing_col in df_base.columns:\n",
    "        df_base = df_base.with_columns(\n",
    "            (pl.col(deposit_incoming_col) / pl.col(deposit_outgoing_col)).alias(\"deposit_ratio\")\n",
    "        )\n",
    "    \n",
    "    # Feature engineering: Date of birth related features (calculating age only)\n",
    "    date_of_birth_cols = [\"dateofbirth_337D\", \"dateofbirth_342D\"]\n",
    "    \n",
    "    for col in date_of_birth_cols:\n",
    "        if col in df_base.columns:\n",
    "            df_base = df_base.with_columns(\n",
    "                pl.col(col).cast(pl.Date).dt.year().apply(calculate_age_from_year, return_dtype=pl.Int64).alias(f\"{col}_age\")\n",
    "            )\n",
    "    \n",
    "    # Feature engineering: Combine credit bureau queries\n",
    "    query_cols = [\"days120_123L\", \"days180_256L\", \"days30_165L\", \"days360_512L\", \"days90_310L\"]\n",
    "    \n",
    "    if all(col in df_base.columns for col in query_cols):\n",
    "        df_base = df_base.with_columns(\n",
    "            sum([pl.col(col) for col in query_cols]).alias(\"total_queries\")\n",
    "        )\n",
    "    \n",
    "    # Feature engineering: Aggregate installment payment percentages\n",
    "    early_payment_col = \"pctinstlsallpaidearl3d_427L\"\n",
    "    late_payment_cols = [\n",
    "        \"pctinstlsallpaidlat10d_839L\",\n",
    "        \"pctinstlsallpaidlate1d_3546856L\",\n",
    "        \"pctinstlsallpaidlate4d_3546849L\",\n",
    "        \"pctinstlsallpaidlate6d_3546844L\"\n",
    "    ]\n",
    "    \n",
    "    if early_payment_col in df_base.columns:\n",
    "        df_base = df_base.with_columns(\n",
    "            pl.col(early_payment_col).alias(\"total_early_payments\")\n",
    "        )\n",
    "    \n",
    "    if all(col in df_base.columns for col in late_payment_cols):\n",
    "        df_base = df_base.with_columns(\n",
    "            sum([pl.col(col) for col in late_payment_cols]).alias(\"total_late_payments\")\n",
    "        )\n",
    "    \n",
    "    # Feature engineering: Combine education levels\n",
    "    edu1_col = \"education_88M\"\n",
    "    edu2_col = \"education_927M\"\n",
    "    \n",
    "    if edu1_col in df_base.columns and edu2_col in df_base.columns:\n",
    "        df_base = df_base.with_columns(\n",
    "            pl.struct([pl.col(edu1_col), pl.col(edu2_col)]).apply(lambda row: combine_education_levels(row[edu1_col], row[edu2_col]), return_dtype=pl.Utf8).alias(\"combined_education\")\n",
    "        )\n",
    "    \n",
    "    # Handle dates if necessary\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    \n",
    "    # Check if the total_credit_limit feature is included\n",
    "    if \"total_credit_limit\" in df_base.columns:\n",
    "        print(\"total_credit_limit feature is included.\")\n",
    "    else:\n",
    "        print(\"total_credit_limit feature is not included.\")\n",
    "        \n",
    "    # Check if the deposit_ratio feature is included\n",
    "    if \"deposit_ratio\" in df_base.columns:\n",
    "        print(\"deposit_ratio feature is included.\")\n",
    "    else:\n",
    "        print(\"deposit_ratio feature is not included.\")\n",
    "        \n",
    "    # Check if the date of birth age features are included\n",
    "    for col in date_of_birth_cols:\n",
    "        if f\"{col}_age\" in df_base.columns:\n",
    "            print(f\"{col}_age feature is included.\")\n",
    "        else:\n",
    "            print(f\"{col}_age feature is not included.\")\n",
    "    \n",
    "    # Check if the credit bureau queries feature is included\n",
    "    if \"total_queries\" in df_base.columns:\n",
    "        print(\"total_queries feature is included.\")\n",
    "    else:\n",
    "        print(\"total_queries feature is not included.\")\n",
    "        \n",
    "    # Check if the installment payment percentage features are included\n",
    "    if \"total_early_payments\" in df_base.columns:\n",
    "        print(\"total_early_payments feature is included.\")\n",
    "    else:\n",
    "        print(\"total_early_payments feature is not included.\")\n",
    "    \n",
    "    if \"total_late_payments\" in df_base.columns:\n",
    "        print(\"total_late_payments feature is included.\")\n",
    "    else:\n",
    "        print(\"total_late_payments feature is not included.\")\n",
    "        \n",
    "    # Check if the combined education feature is included\n",
    "    if \"combined_education\" in df_base.columns:\n",
    "        print(\"combined_education feature is included.\")\n",
    "    else:\n",
    "        print(\"combined_education feature is not included.\")\n",
    "        \n",
    "    return df_base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to convert to pandas format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    \n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    \n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    \n",
    "    return df_data, cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading for the train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "TRAIN_DIR = Path(\"../parquet_files/train\")\n",
    "TEST_DIR = Path(\"../parquet_files/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = feature_eng(**data_store)\n",
    "print(\"train data shape:\\t\", df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = feature_eng(**data_store)\n",
    "print(\"test data shape:\\t\", df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Filtering and Aligning Train and Test Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data to pandas and filter\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train_filtered = df_train.pipe(Pipeline.filter_cols)\n",
    "\n",
    "# Convert test data to pandas and filter\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "df_test_filtered = df_test.pipe(Pipeline.filter_cols)\n",
    "\n",
    "\n",
    "# Print the number of columns left\n",
    "print(\"Number of columns left before filtering:\", len(df_train_filtered.columns))\n",
    "print(\"Number of columns left before filtering:\", len(df_test_filtered.columns))\n",
    "\n",
    "\n",
    "# Ensure that both train and test data have the same columns except for the target attribute\n",
    "common_columns = list(set(df_train_filtered.columns).intersection(set(df_test_filtered.columns)))\n",
    "\n",
    "# Add specific columns to the common columns list if they are not already present\n",
    "columns_to_keep = ['case_id', 'WEEK_NUM', 'month_decision', 'weekday_decision']\n",
    "for col in columns_to_keep:\n",
    "    if col not in common_columns:\n",
    "        common_columns.append(col)\n",
    "\n",
    "# Preserve the original order of columns in df_train_filtered\n",
    "common_columns = [col for col in df_train_filtered.columns if col in common_columns]\n",
    "\n",
    "# Add 'target' column to train data\n",
    "df_train_filtered = df_train_filtered[common_columns + ['target']]\n",
    "\n",
    "# Filter test data to include only common columns\n",
    "df_test_filtered = df_test_filtered[common_columns]\n",
    "\n",
    "# Print the number of columns left\n",
    "print(\"Number of columns left after filtering:\", len(df_train_filtered.columns))\n",
    "print(\"Number of columns left after filtering:\", len(df_test_filtered.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the uncessary memory associated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del data_store\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of numerical columns\n",
    "# numerical_columns = df_train_filtered.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# numerical_columns.remove('target')  # Remove the target column from the numerical columns list\n",
    "\n",
    "# # Plot box plots for numerical columns\n",
    "# for col in numerical_columns:\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     sns.boxplot(x='target', y=col, data=df_train_filtered)\n",
    "#     plt.title(f\"{col} vs Target\")\n",
    "#     plt.xlabel('Target')\n",
    "#     plt.ylabel(col)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_correlation(df, target_column):\n",
    "#     # Ensure the target column is numeric\n",
    "#     if not pd.api.types.is_numeric_dtype(df[target_column]):\n",
    "#         raise ValueError(f\"The target column '{target_column}' is not numeric.\")\n",
    "\n",
    "#     # Select only numeric columns\n",
    "#     numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "#     # Calculate correlation matrix with respect to the target variable\n",
    "#     corr_with_target = numeric_df.corrwith(df[target_column])\n",
    "\n",
    "#     # Filter correlations greater than 0.1\n",
    "#     filtered_corr = corr_with_target[corr_with_target.abs() > 0.01]\n",
    "\n",
    "#     # Plot heatmap of correlation with the target variable\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     sns.heatmap(filtered_corr.to_frame(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "#     plt.title('Correlation Heatmap with Target (|correlation| > 0.1)')\n",
    "#     plt.show()\n",
    "\n",
    "#     # Return filtered correlation with target\n",
    "#     return filtered_corr\n",
    "\n",
    "# correlation_with_target = calculate_correlation(df_train_filtered, 'target')\n",
    "# print(correlation_with_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train is duplicated:\\t\", df_train_filtered[\"case_id\"].duplicated().any())\n",
    "print(\"Test is duplicated:\\t\", df_test_filtered[\"case_id\"].duplicated().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lineplot(\n",
    "#     data=df_train,\n",
    "#     x=\"WEEK_NUM\",\n",
    "#     y=\"target\",\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Define your data and models\n",
    "X = df_train_filtered.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "y = df_train_filtered[\"target\"]\n",
    "weeks = df_train_filtered[\"WEEK_NUM\"]\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedGroupKFold(n_splits=5, shuffle=False)\n",
    "\n",
    "# Define LGBM parameters\n",
    "params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"auc\",\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"colsample_bytree\": 0.8, \n",
    "    \"colsample_bynode\": 0.8,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \n",
    "    #\"class_weight\": \"balanced\",\n",
    "    #\"device\": \"gpu\",\n",
    "}\n",
    "\n",
    "# Train LGBM models\n",
    "fitted_models = []\n",
    "for idx_train, idx_valid in cv.split(X, y, groups=weeks):\n",
    "    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        callbacks=[lgb.log_evaluation(100), lgb.early_stopping(100)]\n",
    "    )\n",
    "\n",
    "    fitted_models.append(model)\n",
    "\n",
    "# Define VotingModel class\n",
    "class VotingModel:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # Average the probabilities predicted by each model\n",
    "        probas = [model.predict_proba(X) for model in self.models]\n",
    "        avg_proba = np.mean(probas, axis=0)\n",
    "        return avg_proba\n",
    "\n",
    "# Instantiate the voting model\n",
    "model = VotingModel(fitted_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train data to pandas and filter\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train_filtered = df_train.pipe(Pipeline.filter_cols)\n",
    "\n",
    "# Convert test data to pandas and filter\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "df_test_filtered = df_test.pipe(Pipeline.filter_cols)\n",
    "\n",
    "# Ensure that both train and test data have the same columns except for the target attribute\n",
    "common_columns = list(set(df_train_filtered.columns).intersection(set(df_test_filtered.columns)))\n",
    "\n",
    "# Add specific columns to the common columns list if they are not already present\n",
    "columns_to_keep = ['case_id', 'WEEK_NUM', 'month_decision', 'weekday_decision']\n",
    "for col in columns_to_keep:\n",
    "    if col not in common_columns:\n",
    "        common_columns.append(col)\n",
    "\n",
    "# Preserve the original order of columns in df_train_filtered\n",
    "common_columns = [col for col in df_train_filtered.columns if col in common_columns]\n",
    "\n",
    "# Add 'target' column to train data\n",
    "df_train_filtered = df_train_filtered[common_columns + ['target']]\n",
    "\n",
    "# Filter test data to include only common columns\n",
    "df_test_filtered = df_test_filtered[common_columns]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate models\n",
    "calibrated_models = []\n",
    "for model in fitted_models:\n",
    "    calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "    calibrated_model.fit(X_train, y_train)\n",
    "    calibrated_models.append(calibrated_model)\n",
    "\n",
    "# Select the final calibrated model (the last one in the list)\n",
    "final_calibrated_model = calibrated_models[-1]\n",
    "\n",
    "# Compute probabilities using the final calibrated model\n",
    "prob_pos_final = final_calibrated_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# Compute calibration curve for the final model\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_valid, prob_pos_final, n_bins=10)\n",
    "\n",
    "# Plot the calibration curve for the final model\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, marker='o', label='Final Calibrated Model')\n",
    "\n",
    "# Plot the reference line (perfect calibration)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black', label='Perfectly calibrated')\n",
    "\n",
    "# Set plot labels and legend\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission for the kaggle and score display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test_filtered.drop(columns=[\"WEEK_NUM\"])\n",
    "X_test = X_test.set_index(\"case_id\")\n",
    "\n",
    "y_pred = pd.Series(model.predict_proba(X_test)[:, 1], index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the training set to calculate the Gini coefficients\n",
    "y_pred_train = pd.Series(model.predict_proba(X)[:, 1], index=X.index)\n",
    "\n",
    "# Functions to calculate Gini coefficient\n",
    "def gini(actual, pred):\n",
    "    assert len(actual) == len(pred)\n",
    "    all_data = np.c_[actual, pred, np.arange(len(actual))]\n",
    "    all_data = all_data[np.lexsort((all_data[:, 2], -1 * all_data[:, 1]))]\n",
    "    total_losses = all_data[:, 0].sum()\n",
    "    gini_sum = all_data[:, 0].cumsum().sum() / total_losses\n",
    "\n",
    "    gini_sum -= (len(actual) + 1) / 2.\n",
    "    return gini_sum / len(actual)\n",
    "\n",
    "def normalized_gini(actual, pred):\n",
    "    return gini(actual, pred) / gini(actual, actual)\n",
    "\n",
    "# Calculate Gini scores per week using training data\n",
    "def calculate_weekly_gini(df_train_f, y_pred_train):\n",
    "    weekly_gini = []\n",
    "    for week in df_train_f[\"WEEK_NUM\"].unique():\n",
    "        week_data = df_train_f[df_train_f[\"WEEK_NUM\"] == week]\n",
    "        actual = week_data[\"target\"]\n",
    "        pred = y_pred_train.loc[week_data.index]\n",
    "        weekly_gini.append(normalized_gini(actual, pred))\n",
    "    return weekly_gini\n",
    "\n",
    "# Calculate stability metric\n",
    "weekly_gini_scores = calculate_weekly_gini(df_train_filtered_scaled, y_pred_train)\n",
    "\n",
    "# Fit linear regression\n",
    "weeks = df_train_filtered_scaled[\"WEEK_NUM\"].unique()\n",
    "slope, intercept, r_value, p_value, std_err = linregress(weeks, weekly_gini_scores)\n",
    "falling_rate = min(0, slope)\n",
    "\n",
    "# Calculate residuals and their standard deviation\n",
    "predicted_gini_scores = intercept + slope * np.array(weeks)\n",
    "residuals = weekly_gini_scores - predicted_gini_scores\n",
    "std_residuals = np.std(residuals)\n",
    "\n",
    "# Calculate mean Gini\n",
    "mean_gini = np.mean(weekly_gini_scores)\n",
    "\n",
    "# Calculate stability metric\n",
    "stability_metric = mean_gini + 88.0 * falling_rate - 0.5 * std_residuals\n",
    "\n",
    "print(f'Stability Metric: {stability_metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm = pd.read_csv(\"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "\n",
    "df_subm[\"score\"] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Check null: \", df_subm[\"score\"].isnull().any())\n",
    "\n",
    "df_subm_sorted = df_subm.sort_values(by=\"score\", ascending=False)\n",
    "top_5_scores = df_subm_sorted.head()\n",
    "print(top_5_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
